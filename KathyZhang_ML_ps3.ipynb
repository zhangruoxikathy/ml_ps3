{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> PPHA 30546 Machine Learning | PS3</font> \n",
    "\n",
    "Kathy Zhang, Yimeng Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T02:10:13.296809Z",
     "start_time": "2024-01-16T02:10:10.114201Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sympy import symbols, Eq, solve, exp\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CH5 Q6\n",
    "#### 6. We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the sm.GLM() function. Do not forget to set a random seed before beginning your analysis. \n",
    "\n",
    "• In parts (a) and (d), the problem references the sm.glm() function. You can use the sm.Logit() function instead if you’d prefer. The sm.glm() function is more general way to estimate different linear models (hence the GLM name), but it will estimate a logistic regression with the appropriate arguments.\n",
    "\n",
    "• Part (a) also references the summarize() function. This is a typo. The authors mean the .summary() method as a way to view the results of a sm.glm() fit.\n",
    "\n",
    "• In part (c), please draw 1,000 bootstrap samples when bootstrapping your standard errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Using the summarize() and sm.GLM() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> We first assigned dummies for default and student. The estimated SE for balance is 0, for income is 4.99e-06, for constant is 0.435. Coefficient estimates for the above are 0.0056, 2.081e-05, and -11.5405 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance        income\n",
       "0      No      No   729.526495  44361.625074\n",
       "1      No     Yes   817.180407  12106.134700\n",
       "2      No      No  1073.549164  31767.138947\n",
       "3      No      No   529.250605  35704.493935\n",
       "4      No      No   785.655883  38463.495879"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default = pd.read_csv('Data-Default.csv')\n",
    "print(default.shape, '\\n')\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.138947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.495879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default  student      balance        income\n",
       "0        0        0   729.526495  44361.625074\n",
       "1        0        1   817.180407  12106.134700\n",
       "2        0        0  1073.549164  31767.138947\n",
       "3        0        0   529.250605  35704.493935\n",
       "4        0        0   785.655883  38463.495879"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make default and student dummy variables\n",
    "encoding_dict = {'Yes': 1,'No': 0}\n",
    "for col in ['default', 'student']:\n",
    "    default[col] = default[col].map(encoding_dict)\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                default   No. Observations:                10000\n",
      "Model:                            GLM   Df Residuals:                     9997\n",
      "Model Family:                Binomial   Df Model:                            2\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -789.48\n",
      "Date:                Tue, 13 Feb 2024   Deviance:                       1579.0\n",
      "Time:                        15:07:11   Pearson chi2:                 6.95e+03\n",
      "No. Iterations:                     9   Pseudo R-squ. (CS):             0.1256\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -11.5405      0.435    -26.544      0.000     -12.393     -10.688\n",
      "balance        0.0056      0.000     24.835      0.000       0.005       0.006\n",
      "income      2.081e-05   4.99e-06      4.174      0.000     1.1e-05    3.06e-05\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1) # set a random seed\n",
    "\n",
    "# Build the logistic regression model\n",
    "X = default[['balance', 'income']]\n",
    "X = sm.add_constant(X)\n",
    "y = default['default']\n",
    "\n",
    "results = sm.GLM(y, X, family=sm.families.Binomial()).fit() # binary outcome\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Write a function, boot_fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 235, 5192,  905, 7813, 2895], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_indices(data, n):\n",
    "    '''\n",
    "    Generates a random subsample (i.e., its indices)\n",
    "    with replacement consisting of n observations each.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - n (int): number of observations in the sample\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        - indices (np.ndarray): array of indices forming\n",
    "            the samples\n",
    "    '''\n",
    "    assert type(data) == pd.DataFrame\n",
    "    assert type(n) == int, 'n must be an integer'\n",
    "\n",
    "    indices = np.random.choice(\n",
    "        data.index,         # Indices as the input\n",
    "        int(n),             # Number of indices per sample\n",
    "        replace=True        # Draw samples with replacement\n",
    "    )\n",
    "    return indices\n",
    "\n",
    "get_indices(default, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of a single subsample:\n",
      "\tIntercept:\t-11.65874\n",
      "\tBalance:\t0.00575\n",
      "\tIncome:\t\t1.919e-05\n"
     ]
    }
   ],
   "source": [
    "def boot_fn(data, index, constant=True, features=['balance','income'],\n",
    "            target='default'):\n",
    "    '''\n",
    "    Runs a logistic regression (with a constant) on only the specified\n",
    "    indices within the data (i.e., on a single subsample).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - indices (np.ndarray): array of indices forming the samples\n",
    "        - features (lst of str): the name of the features\n",
    "        - target (str): the name of the target\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        - coefficients (lst of float): the coefficients\n",
    "    '''\n",
    "    X = data[features].loc[index]\n",
    "    if constant:\n",
    "        X = sm.add_constant(X)\n",
    "    y = data[target].loc[index]\n",
    "    \n",
    "    lr = sm.GLM(y, X, family=sm.families.Binomial()).fit(disp=0)\n",
    "    coefficients = [lr.params[0], lr.params[1], lr.params[2]]\n",
    "\n",
    "    return coefficients\n",
    "\n",
    "intercept, coef_balance, coef_income = boot_fn(default,\n",
    "                                               get_indices(default, 10000))\n",
    "print(f'Coefficients of a single subsample:\\n\\tIntercept:\\t{round(intercept,5)}\\n\\tBalance:\\t{round(coef_balance, 5)}\\n\\tIncome:\\t\\t{round(coef_income, 8)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Following the bootstrap example in the lab, use your boot_fn() function to estimate the standard errors of the logistic regression coefficients for income and balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> Estimated coefficients on balance and income are 0.0057 and 2.102e-05, with standard errors being 0.00023 and 5.035e-06 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot(data, func, B):\n",
    "    '''\n",
    "    Executing a bootstrap over B subsamples\n",
    "    to estimate the (mean of) coefficients\n",
    "    and their associated standard errors.\n",
    "\n",
    "    Inputs:\n",
    "        - data (pd.Dataframe): data to sample from\n",
    "        - func (fn): function executing the regression\n",
    "        - B (int): number of subsamples\n",
    "    \n",
    "    Ouput:\n",
    "        - restults (dict of dicts): bootstrapped coefficients\n",
    "            and the associated standard errors\n",
    "    '''\n",
    "    # Step 4\n",
    "    coef_intercept = []\n",
    "    coef_balance = []\n",
    "    coef_income = []\n",
    "\n",
    "    coefs = ['intercept', 'balance', 'income']\n",
    "    output = {coef: [] for coef in coefs}\n",
    "    for i in range(B):\n",
    "        reg_out = func(data, get_indices(data, len(data)))\n",
    "        for i, coef in enumerate(coefs):\n",
    "            output[coef].append(reg_out[i])\n",
    "\n",
    "    # Step 5\n",
    "    results = {}\n",
    "    for coef in coefs:\n",
    "        results[coef] = {\n",
    "            'estimate': np.mean(output[coef]),\n",
    "            'std_err': np.std(output[coef])\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:\n",
      "\tEstimate: -11.573858754859323\n",
      "\tStandard Error: 0.448417021813344\n",
      "Balance:\n",
      "\tEstimate: 0.005662047099121872\n",
      "\tStandard Error: 0.00023244529100075235\n",
      "Income:\n",
      "\tEstimate: 2.102052289223684e-05\n",
      "\tStandard Error: 5.035000021103877e-06\n"
     ]
    }
   ],
   "source": [
    "results = boot(default, boot_fn, 1000)\n",
    "for i, x in results.items():\n",
    "    print(f\"{i.capitalize()}:\\n\\tEstimate: {x['estimate']}\\n\\tStandard Error: {x['std_err']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Comment on the estimated standard errors obtained using the sm.GLM() function and using the bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> The estimated SEs obtained using boostrap (0.00023, 5.035e-6) are a little bit higher that those using sm.GLM() function (0, 4.99e-0.6) but very close to each other (they equalize at 0.001 level), indicating that bootstraping provides robust and reliable estimates SEs for coefficients from a logistic regression fit. By using bootstrap, we reduce the risk of overfitting and improve the stability of our machine learning algorithms.\n",
    "\n",
    "<font color='blue'>The standard forms using sm.GLM() depend on the unknown parameter, such as the noise variance $ \\sigma^2 $. Secondly, standard formulas assume that xi are fixed, and all the variability comes from the variation in the error terms. The bootstrap approach does not rely on any of these assumptions. The bootstrap method, being a non-parametric approach that makes fewer assumptions, may be providing a more accurate reflection of the uncertainty associated with each coefficient estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CH5 Q8\n",
    "\n",
    "#### 8. We will now perform cross-validation on a simulated data set.\n",
    "\n",
    "• In part (a), note that the code you’re given sets a random seed equal to 1.\n",
    "\n",
    "• In part (c), please keep the same random seed as in part (a).\n",
    "\n",
    "• In part (d), please set a random seed equal to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Generate a simulated data set as follows:\n",
    "`rng = np.random.default_rng(1)`\n",
    "\n",
    "`x = rng.normal(size=100)`\n",
    "\n",
    "`y = x - 2 * x**2 + rng.normal(size=100)`\n",
    "\n",
    "#### In this data set, what is n and what is p? Write out the model used to generate the data in equation form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> n is number of observations = 100, p is the number of predictors = 2. The true model is:\n",
    "\n",
    "<font color='blue'> $ Y = X - 2X^2 + ϵ$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # set a random seed\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "x = rng.normal(size=100)\n",
    "y = x - 2 * x**2 + rng.normal(size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Create a scatterplot of X against Y . Comment on what you find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> The plot shows a quadratic, negative convex relationship between X and Y, where Y first increases and then decreases when X reaches ~0 as X increases. X ranges from -2 to 2 and Y ranges from -15 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2128079f090>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:\n",
    "\n",
    "i. $ Y = B0 + B1X + ϵ $\n",
    "\n",
    "ii. $ Y = B0 + B1X + B2X^2 + ϵ $ \n",
    "\n",
    "iii. $ Y = B0 + B1X + B2X^2 + B3X^3 + ϵ $\n",
    "\n",
    "iv. $ Y = B0 + B1X + B2X^2 + B3X^3 + B4X^4 + ϵ $.\n",
    "\n",
    "#### Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> The LOOCV errors are calculated below.\n",
    "\n",
    "<font color='blue'> The results are:\n",
    "\n",
    "- <font color='blue'>Model 1: MSE = 6.633029839181983\n",
    "- <font color='blue'>Model 2: MSE = 1.1229368563419677\n",
    "- <font color='blue'>Model 3: MSE = 1.3017965489358883\n",
    "- <font color='blue'>Model 4: MSE = 1.3323942694179352 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'x': x,\n",
    "    'x_2': np.power(x, 2),\n",
    "    'x_3': np.power(x, 3),\n",
    "    'x_4': np.power(x, 4),\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.345584</td>\n",
       "      <td>0.119428</td>\n",
       "      <td>0.041273</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>-0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.821618</td>\n",
       "      <td>0.675056</td>\n",
       "      <td>0.554639</td>\n",
       "      <td>0.455701</td>\n",
       "      <td>0.333950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.330437</td>\n",
       "      <td>0.109189</td>\n",
       "      <td>0.036080</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>-0.013532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.303157</td>\n",
       "      <td>1.698219</td>\n",
       "      <td>-2.213046</td>\n",
       "      <td>2.883947</td>\n",
       "      <td>-4.030442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.905356</td>\n",
       "      <td>0.819669</td>\n",
       "      <td>0.742092</td>\n",
       "      <td>0.671858</td>\n",
       "      <td>0.484861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x       x_2       x_3       x_4         y\n",
       "0  0.345584  0.119428  0.041273  0.014263 -0.544554\n",
       "1  0.821618  0.675056  0.554639  0.455701  0.333950\n",
       "2  0.330437  0.109189  0.036080  0.011922 -0.013532\n",
       "3 -1.303157  1.698219 -2.213046  2.883947 -4.030442\n",
       "4  0.905356  0.819669  0.742092  0.671858  0.484861"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: MSE = 6.633029839181983\n",
      "Model 2: MSE = 1.1229368563419677\n",
      "Model 3: MSE = 1.3017965489358883\n",
      "Model 4: MSE = 1.3323942694179352\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1) # set a random seed\n",
    "\n",
    "models = {\n",
    "    'Model 1': ['x'],\n",
    "    'Model 2': ['x', 'x_2'],\n",
    "    'Model 3': ['x', 'x_2', 'x_3'],\n",
    "    'Model 4': ['x', 'x_2', 'x_3', 'x_4']\n",
    "}\n",
    "\n",
    "def calculate_mse_for_models(df, models):\n",
    "    mse_results = {}\n",
    "\n",
    "    for model_name, features in models.items():\n",
    "        squared_errors = []\n",
    "        \n",
    "        for i in df.index:\n",
    "            # Split the data into training and testing sets\n",
    "            train = df.iloc[df.index != i]\n",
    "            test = df.iloc[df.index == i]\n",
    "\n",
    "            # Fit the model and gather the squared error\n",
    "            ols = LinearRegression().fit(train[features], train['y'])\n",
    "            test_predicted = ols.predict(test[features])\n",
    "            test_actual = test['y']\n",
    "            squared_error = np.power(test_predicted - test_actual, 2)\n",
    "            \n",
    "            # Store the squared error\n",
    "            squared_errors.append(squared_error.values[0])\n",
    "        \n",
    "        # Calculate the MSE using LOOCV for the model\n",
    "        mse_results[model_name] = np.mean(squared_errors)\n",
    "\n",
    "    # Print the MSE results for each model\n",
    "    for model_name, mse in mse_results.items():\n",
    "        print(f'{model_name}: MSE = {mse}')\n",
    "        \n",
    "\n",
    "calculate_mse_for_models(df, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> The results are exactly the same using random seed = 1 and 2. This consistency arises because only a single observation is excluded from the training set in each iteration of the LOOCV process. Consequently, the selection of observations for the test set does not introduce any randomness into the procedure. Therefore, the LOOCV results will be invariant to the choice of random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: MSE = 6.633029839181983\n",
      "Model 2: MSE = 1.1229368563419677\n",
      "Model 3: MSE = 1.3017965489358883\n",
      "Model 4: MSE = 1.3323942694179352\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2) # set a different random seed\n",
    "\n",
    "calculate_mse_for_models(df, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> Model 2 had the smallest LOOCV error of 1.12, which is indeed what we expected since we created y based on exactly Model 2, the quadratic form. Model 1 has the highest MSE of 6.63 since it does not account for quadratic, non-linear relationships, then Model 2 has the least MSE, while MSE increases going from Model 2 to 3 to 1.30, 3 to 4 to 1.33 since x^3 and x^4 are irrelevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> \n",
    "t-stats and p-values for each ceofficient estimates in below models:\n",
    "\n",
    "| t-stats/p-values   | Model 1   | Model 2    | Model 3   | Model 4   |\n",
    "|-------------|-------------|-------------|-------------|-------------|\n",
    "| $ x $ | 6.752/0 | 7.647/0  | 5.945/0 | 4.423/0|\n",
    "| $ x^2 $ |  | -22.072/0 | -20.673/0 | -11.336/0 | \n",
    "| $ x^3 $ |  |  | -1.07/0.287 | 0.466/0.642 |\n",
    "| $ x^4 $ |  |  |  | 2.309/0.023 |\n",
    "\n",
    "Significance of $ x^2 > x > x^4 > x^3 $\n",
    "\n",
    "Model 2 still has the highest statistical significance with both p-values for its coefficient estimates with best second degree variable followed by first degree variable based on t-stats. \n",
    "\n",
    "It is notable that all coefficient estimates on $ x $ and $ x^2 $ for all models are statistically significant with p values < 0.05 at 95% confidence level, and Model 3 and 4 do not have statistically significant $ x^3 $ at 95% confidence level, Model 4 does not have statistically significant $ x^4 $ at 98% confidence level. For each of the models, the absolute t-stats on ceofficient estimate $ x^2 $ are also the highest--22, 21, 11, and it decreases as we add more irrelevant variables. \n",
    "Hence, these results agree with the conclusions drawn based on the cross-validation results that $ x $ and $ x^2 $ are variables with relevance and significance for the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.318\n",
      "Model:                            OLS   Adj. R-squared:                  0.311\n",
      "Method:                 Least Squares   F-statistic:                     45.60\n",
      "Date:                Tue, 13 Feb 2024   Prob (F-statistic):           1.04e-09\n",
      "Time:                        15:07:30   Log-Likelihood:                -230.83\n",
      "No. Observations:                 100   AIC:                             465.7\n",
      "Df Residuals:                      98   BIC:                             470.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.4650      0.247     -5.937      0.000      -1.955      -0.975\n",
      "x              1.9494      0.289      6.752      0.000       1.376       2.522\n",
      "==============================================================================\n",
      "Omnibus:                       52.788   Durbin-Watson:                   1.972\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              149.089\n",
      "Skew:                          -1.953   Prob(JB):                     4.22e-33\n",
      "Kurtosis:                       7.530   Cond. No.                         1.20\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Model 2\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.887\n",
      "Model:                            OLS   Adj. R-squared:                  0.884\n",
      "Method:                 Least Squares   F-statistic:                     379.5\n",
      "Date:                Tue, 13 Feb 2024   Prob (F-statistic):           1.36e-46\n",
      "Time:                        15:07:30   Log-Likelihood:                -141.06\n",
      "No. Observations:                 100   AIC:                             288.1\n",
      "Df Residuals:                      97   BIC:                             295.9\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0728      0.119     -0.611      0.543      -0.309       0.164\n",
      "x              0.9663      0.126      7.647      0.000       0.715       1.217\n",
      "x_2           -2.0047      0.091    -22.072      0.000      -2.185      -1.824\n",
      "==============================================================================\n",
      "Omnibus:                        1.338   Durbin-Watson:                   2.197\n",
      "Prob(Omnibus):                  0.512   Jarque-Bera (JB):                0.814\n",
      "Skew:                           0.119   Prob(JB):                        0.666\n",
      "Kurtosis:                       3.372   Cond. No.                         2.23\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Model 3\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.888\n",
      "Model:                            OLS   Adj. R-squared:                  0.885\n",
      "Method:                 Least Squares   F-statistic:                     253.8\n",
      "Date:                Tue, 13 Feb 2024   Prob (F-statistic):           1.70e-45\n",
      "Time:                        15:07:30   Log-Likelihood:                -140.47\n",
      "No. Observations:                 100   AIC:                             288.9\n",
      "Df Residuals:                      96   BIC:                             299.4\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0572      0.120     -0.477      0.635      -0.295       0.181\n",
      "x              1.1146      0.187      5.945      0.000       0.742       1.487\n",
      "x_2           -2.0471      0.099    -20.673      0.000      -2.244      -1.851\n",
      "x_3           -0.0643      0.060     -1.070      0.287      -0.184       0.055\n",
      "==============================================================================\n",
      "Omnibus:                        0.845   Durbin-Watson:                   2.199\n",
      "Prob(Omnibus):                  0.655   Jarque-Bera (JB):                0.392\n",
      "Skew:                           0.052   Prob(JB):                        0.822\n",
      "Kurtosis:                       3.289   Cond. No.                         5.95\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Model 4\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.894\n",
      "Model:                            OLS   Adj. R-squared:                  0.890\n",
      "Method:                 Least Squares   F-statistic:                     200.2\n",
      "Date:                Tue, 13 Feb 2024   Prob (F-statistic):           2.22e-45\n",
      "Time:                        15:07:30   Log-Likelihood:                -137.74\n",
      "No. Observations:                 100   AIC:                             285.5\n",
      "Df Residuals:                      95   BIC:                             298.5\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1008      0.136      0.743      0.460      -0.169       0.370\n",
      "x              0.9050      0.205      4.423      0.000       0.499       1.311\n",
      "x_2           -2.5059      0.221    -11.336      0.000      -2.945      -2.067\n",
      "x_3            0.0338      0.073      0.466      0.642      -0.110       0.178\n",
      "x_4            0.1042      0.045      2.309      0.023       0.015       0.194\n",
      "==============================================================================\n",
      "Omnibus:                        2.476   Durbin-Watson:                   2.163\n",
      "Prob(Omnibus):                  0.290   Jarque-Bera (JB):                2.097\n",
      "Skew:                           0.118   Prob(JB):                        0.351\n",
      "Kurtosis:                       3.669   Cond. No.                         19.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Print summary table for each regression\n",
    "for model_name, features in models.items():\n",
    "        model = sm.OLS(df['y'], sm.add_constant(df[features]))\n",
    "        results = model.fit()\n",
    "        print(model_name)\n",
    "        print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CH6 Q11\n",
    "\n",
    "#### 11. We will now try to predict per capita crime rate in the Boston dataset.\n",
    "\n",
    "• In part (a), use forward stepwise & backwards stepwise selection (FSS & BSS) instead of the methods the book lists. Do so based on using a mathematical adjustment approach (AIC) and 5-Fold Cross-Validation (5FCV) to estimate the test error. Use the entire dataset for 5FCV, shuffle the data randomly for splitting, and set random_state=23). This means you will select a model four different ways: FSS-AIC, FSS-5FCV, BSS-AIC, BSS-5FCV.\n",
    "\n",
    "• As part of your answer for part (b), be sure to explain why the the different methods you use may select different models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='blue'> The above model selection methods generated the below four models with relevant statistics:\n",
    "\n",
    "- <font color='blue'>FSS-AIC: ['constant', 'RAD', 'LSTAT', 'B', 'MDEV', 'ZN', 'DIS', 'NOX', 'PTRATIO'] (8 VARIABLES)\n",
    "\n",
    "    <font color='blue'>AIC: 3332.396301742922\n",
    "\n",
    "- <font color='blue'>FSS-5FCV: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'CHAS'] (9 VARIABLES)\n",
    "\n",
    "    <font color='blue'>Error rate: 42.016041492205616\n",
    "\n",
    "- <font color='blue'>BSS-AIC: ['ZN', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'B', 'LSTAT', 'MDEV', 'constant'] (8 VARIABLES)\n",
    "\n",
    "    <font color='blue'>AIC: 3332.3963017429214\n",
    "\n",
    "- <font color='blue'>BSS-5FCV: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV'] (9 VARIABLES)\n",
    "\n",
    "    <font color='blue'>Error rate: 42.01604149220564\n",
    "\n",
    "\n",
    "<font color='blue'>It is notable that forward or backward stepwise selection does not make a difference--AICs and error rates generated using the two models are exactly the same respectively--3332.4=3332.4, 42.02=42.02. There is very minor differences in AIC and error rates even with the same variables, likely due to error calculations during the forward and backward selection process, but they are in fact the same models.\n",
    "\n",
    "<font color='blue'>Here are some interpretations for AIC, 5FCV, Forward vs Backward Stepwise Selection, and variable selections:\n",
    "\n",
    "- <font color='blue'>AIC (Akaike Information Criterion) is a measure of the relative quality of a statistical model for a given set of data. It balances the complexity of the model against how well the model fits the data (likelihood). Lower AIC values indicate a better model. Both FSS-AIC and BSS-AIC have identified models with similar complexity and fit, as indicated by the nearly identical AIC values. This suggests that both methods converge to a model with the same balance between complexity and fit.\n",
    "\n",
    "- <font color='blue'>5-Fold Cross-Validation (5FCV): This method divides the dataset into 5 parts, trains the model on 4 parts, and tests it on the remaining part. This process is repeated 5 times, with each part used for testing exactly once. The average error rate across all 5 tests provides a robust estimate of the model's predictive performance on unseen data. Lower error rates indicate better predictive performance. The same error rates for FSS-5FCV and BSS-5FCV suggests that both forward and backward selections, when coupled with 5FCV, lead to the same models.\n",
    "\n",
    "- <font color='blue'>The fact that both FSS-5FCV and BSS-5FCV, FSS-AIC and BSS-AIC select the same variables highlights a key point about stepwise selection methods: while the direction of the selection process might influence the order in which variables are added or removed during the model-building process, the final set of chosen variables can be the same if the selection criteria identifies a clear set of variables that contribute most significantly to predicting the target variable.\n",
    "\n",
    "- <font color='blue'>Another finding is more variables, higher flexibility does not imply a lower error, and it is helpful to use these variable selection methods to reduce overfitting, balance between bias and variance. Moreover, certain variables are significant to determining crime rates as they appear in all models, such as `RAD`, `LSTAT`, `MEDEV`, `ZN`, `DIS`, `NOX`, `PTRATIO`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
      "       'PTRATIO', 'B', 'LSTAT', 'MDEV'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MDEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MDEV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "boston = pd.read_csv('Boston.csv')\n",
    "print(boston.columns)\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward stepwise with AIC (FSS-AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best FSS-AIC Model Predictors: ['constant', 'RAD', 'LSTAT', 'B', 'MDEV', 'ZN', 'DIS', 'NOX', 'PTRATIO']\n",
      "Best FSS-AIC Model AIC: 3332.396301742922\n",
      "Best FSS-AIC Model Summary:\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   CRIM   R-squared:                       0.445\n",
      "Model:                            OLS   Adj. R-squared:                  0.436\n",
      "Method:                 Least Squares   F-statistic:                     49.78\n",
      "Date:                Tue, 13 Feb 2024   Prob (F-statistic):           7.23e-59\n",
      "Time:                        15:07:30   Log-Likelihood:                -1657.2\n",
      "No. Observations:                 506   AIC:                             3332.\n",
      "Df Residuals:                     497   BIC:                             3370.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "constant      19.7596      6.114      3.232      0.001       7.748      31.771\n",
      "RAD            0.5334      0.050     10.678      0.000       0.435       0.632\n",
      "LSTAT          0.1075      0.070      1.546      0.123      -0.029       0.244\n",
      "B             -0.0072      0.004     -1.980      0.048      -0.014   -5.72e-05\n",
      "MDEV          -0.1765      0.054     -3.254      0.001      -0.283      -0.070\n",
      "ZN             0.0432      0.018      2.392      0.017       0.008       0.079\n",
      "DIS           -0.9301      0.263     -3.535      0.000      -1.447      -0.413\n",
      "NOX          -12.9796      4.782     -2.714      0.007     -22.375      -3.585\n",
      "PTRATIO       -0.3164      0.184     -1.721      0.086      -0.677       0.045\n",
      "==============================================================================\n",
      "Omnibus:                      665.488   Durbin-Watson:                   1.503\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            84815.611\n",
      "Skew:                           6.594   Prob(JB):                         0.00\n",
      "Kurtosis:                      65.040   Cond. No.                     9.34e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.34e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# FSS-AIC\n",
    "# Add constant to dataframe\n",
    "boston['constant'] = 1 \n",
    "vars_left_add = ['ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',\n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
    "\n",
    "\n",
    "def forward_stepwise_selection_aic(data, vars, target):\n",
    "    \"\"\"Return the best model with the lowest AIC using forward stepwise selection.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): entire dataframe\n",
    "        vars (list): list of variables\n",
    "        target (string): name of the target variable\n",
    "\n",
    "    Returns:\n",
    "        list: best model list that contains selected variables, best AIC, and best model\n",
    "    \"\"\"\n",
    "    remaining_vars = vars[:]\n",
    "    current_vars = ['constant']\n",
    "    best_aic = np.inf\n",
    "    aic_list = []\n",
    "    \n",
    "    for iter in range(len(remaining_vars)):\n",
    "        aic_candidates = []\n",
    "        for var in remaining_vars:\n",
    "            X_train = sm.add_constant(data[current_vars + [var]])\n",
    "            y_train = data[target]\n",
    "            model = sm.OLS(y_train, X_train).fit()\n",
    "            aic_candidates.append((model.aic, var, model))\n",
    "        \n",
    "        aic_candidates.sort()\n",
    "        best_new_aic, best_predictor, best_model = aic_candidates[0]\n",
    "        \n",
    "        if best_new_aic < best_aic:\n",
    "            current_vars.append(best_predictor)\n",
    "            remaining_vars.remove(best_predictor)\n",
    "            best_aic = best_new_aic\n",
    "            aic_list.append((current_vars[:], best_aic, best_model))\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return sorted(aic_list, key=lambda x: x[1])[0]\n",
    "\n",
    "# Run FSS-AIC\n",
    "fss_aic = forward_stepwise_selection_aic(boston, vars_left_add, 'CRIM')\n",
    "\n",
    "# Display the best model\n",
    "fss_aic_best_model_vars, best_fss_aic_score, fss_aic_best_model = fss_aic\n",
    "print(\"Best FSS-AIC Model Predictors:\", fss_aic_best_model_vars)\n",
    "print(\"Best FSS-AIC Model AIC:\", best_fss_aic_score)\n",
    "print(\"Best FSS-AIC Model Summary:\\n\", fss_aic_best_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward stepwise with 5FCV (FSS-5FCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initial run with only one var (constant term/only bias weight): ['constant']\n",
      "      Benchmark error: 73.8391309169969\n",
      "\n",
      "\u001b[1mIteration: 0 \u001b[0m\n",
      " Running model with: ['constant', 'ZN']\n",
      "      Error: 70.960997430059\n",
      " Running model with: ['constant', 'INDUS']\n",
      "      Error: 61.78518850081366\n",
      " Running model with: ['constant', 'CHAS']\n",
      "      Error: 73.6962319501766\n",
      " Running model with: ['constant', 'NOX']\n",
      "      Error: 61.032928313349714\n",
      " Running model with: ['constant', 'RM']\n",
      "      Error: 70.56209468980886\n",
      " Running model with: ['constant', 'AGE']\n",
      "      Error: 64.96223484409609\n",
      " Running model with: ['constant', 'DIS']\n",
      "      Error: 63.47276994624402\n",
      " Running model with: ['constant', 'RAD']\n",
      "      Error: 45.87063742559609\n",
      " Running model with: ['constant', 'TAX']\n",
      "      Error: 49.54812111463557\n",
      " Running model with: ['constant', 'PTRATIO']\n",
      "      Error: 67.79253077321663\n",
      " Running model with: ['constant', 'B']\n",
      "      Error: 65.8945017536434\n",
      " Running model with: ['constant', 'LSTAT']\n",
      "      Error: 59.03812316496426\n",
      " Running model with: ['constant', 'MDEV']\n",
      "      Error: 63.08148281629504\n",
      "          *** Variable selected: RAD\n",
      "          *** Min error selected: 45.87063742559609\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 1 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'ZN']\n",
      "      Error: 45.87233145611557\n",
      " Running model with: ['constant', 'RAD', 'INDUS']\n",
      "      Error: 45.735359797600736\n",
      " Running model with: ['constant', 'RAD', 'CHAS']\n",
      "      Error: 45.76337400811675\n",
      " Running model with: ['constant', 'RAD', 'NOX']\n",
      "      Error: 45.702910399350614\n",
      " Running model with: ['constant', 'RAD', 'RM']\n",
      "      Error: 45.28193004519326\n",
      " Running model with: ['constant', 'RAD', 'AGE']\n",
      "      Error: 45.50260244732577\n",
      " Running model with: ['constant', 'RAD', 'DIS']\n",
      "      Error: 45.4090939520493\n",
      " Running model with: ['constant', 'RAD', 'TAX']\n",
      "      Error: 45.798441349296766\n",
      " Running model with: ['constant', 'RAD', 'PTRATIO']\n",
      "      Error: 45.875188261782924\n",
      " Running model with: ['constant', 'RAD', 'B']\n",
      "      Error: 47.1189837097978\n",
      " Running model with: ['constant', 'RAD', 'LSTAT']\n",
      "      Error: 43.86844997854149\n",
      " Running model with: ['constant', 'RAD', 'MDEV']\n",
      "      Error: 44.040360341685485\n",
      "          *** Variable selected: LSTAT\n",
      "          *** Min error selected: 43.86844997854149\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 2 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'ZN']\n",
      "      Error: 43.68646017811647\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'INDUS']\n",
      "      Error: 43.81227969581819\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'CHAS']\n",
      "      Error: 43.76448937221788\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'NOX']\n",
      "      Error: 43.86745415554707\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'RM']\n",
      "      Error: 43.90423563059539\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'AGE']\n",
      "      Error: 43.90784349798934\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'DIS']\n",
      "      Error: 43.83758500808462\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'TAX']\n",
      "      Error: 43.87204851949356\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'PTRATIO']\n",
      "      Error: 43.777804498088834\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'B']\n",
      "      Error: 45.46612799157435\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV']\n",
      "      Error: 43.61867017584542\n",
      "          *** Variable selected: MDEV\n",
      "          *** Min error selected: 43.61867017584542\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 3 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'ZN']\n",
      "      Error: 43.393606635886655\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'INDUS']\n",
      "      Error: 43.545881068330644\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'CHAS']\n",
      "      Error: 43.6000260807146\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'NOX']\n",
      "      Error: 43.61823447682997\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'RM']\n",
      "      Error: 43.561765199116444\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'AGE']\n",
      "      Error: 43.66261749073045\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'DIS']\n",
      "      Error: 43.51606913598135\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'TAX']\n",
      "      Error: 43.59451517468373\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO']\n",
      "      Error: 43.322270701347264\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'B']\n",
      "      Error: 45.267418828321695\n",
      "          *** Variable selected: PTRATIO\n",
      "          *** Min error selected: 43.322270701347264\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 4 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN']\n",
      "      Error: 43.20315977472645\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'INDUS']\n",
      "      Error: 43.26655211258846\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'CHAS']\n",
      "      Error: 43.28148194113676\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'NOX']\n",
      "      Error: 43.25668323526061\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'RM']\n",
      "      Error: 43.304876401637685\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'AGE']\n",
      "      Error: 43.37056100909691\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'DIS']\n",
      "      Error: 43.20828199790081\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'TAX']\n",
      "      Error: 43.295848659417096\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'B']\n",
      "      Error: 45.09615921142439\n",
      "          *** Variable selected: ZN\n",
      "          *** Min error selected: 43.20315977472645\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 5 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'INDUS']\n",
      "      Error: 43.20209465268819\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'CHAS']\n",
      "      Error: 43.177940979489264\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'NOX']\n",
      "      Error: 43.2139353924738\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'RM']\n",
      "      Error: 43.192627221244464\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'AGE']\n",
      "      Error: 43.19825630460575\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS']\n",
      "      Error: 42.63806711691559\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'TAX']\n",
      "      Error: 43.17732633995576\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'B']\n",
      "      Error: 44.95482067269681\n",
      "          *** Variable selected: DIS\n",
      "          *** Min error selected: 42.63806711691559\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 6 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'INDUS']\n",
      "      Error: 42.34754567668274\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'CHAS']\n",
      "      Error: 42.57981567664349\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX']\n",
      "      Error: 42.12628970994573\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'RM']\n",
      "      Error: 42.66487980501123\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'AGE']\n",
      "      Error: 42.653935228706\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'TAX']\n",
      "      Error: 42.4846425466323\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'B']\n",
      "      Error: 44.45385587774513\n",
      "          *** Variable selected: NOX\n",
      "          *** Min error selected: 42.12628970994573\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 7 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS']\n",
      "      Error: 42.027180979975796\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'CHAS']\n",
      "      Error: 42.104632006276546\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'RM']\n",
      "      Error: 42.113192522230506\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'AGE']\n",
      "      Error: 42.1817873091765\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'TAX']\n",
      "      Error: 42.08039593244892\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'B']\n",
      "      Error: 43.863951190491846\n",
      "          *** Variable selected: INDUS\n",
      "          *** Min error selected: 42.027180979975796\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 8 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'CHAS']\n",
      "      Error: 42.016041492205616\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'RM']\n",
      "      Error: 42.037325940591735\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'AGE']\n",
      "      Error: 42.074113865213405\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'TAX']\n",
      "      Error: 42.03894148273887\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'B']\n",
      "      Error: 43.76119477250578\n",
      "          *** Variable selected: CHAS\n",
      "          *** Min error selected: 42.016041492205616\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 9 \u001b[0m\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'CHAS', 'RM']\n",
      "      Error: 42.0408344404023\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'CHAS', 'AGE']\n",
      "      Error: 42.055162767870236\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'CHAS', 'TAX']\n",
      "      Error: 42.01828777442638\n",
      " Running model with: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'CHAS', 'B']\n",
      "      Error: 43.773894930504454\n",
      "          \u001b[4m*** No variable was selected \u001b[0m\n",
      "          *** Previous error rate ( 42.016041492205616 ) is lower than smallest error rate of this iteration ( 42.01828777442638 )\n",
      "          *** Break\n",
      "\n",
      "Best FSS-5FCV Model Predictors: ['constant', 'RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'CHAS']\n",
      "Error rate: 42.016041492205616\n"
     ]
    }
   ],
   "source": [
    "# FSS-5FCV\n",
    "\n",
    "def forward_stepwise_selection_5fcv(data, vars, target):\n",
    "    \"\"\"Return the best model with the lowest 5fcv using forward stepwise selection.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): entire dataframe\n",
    "        vars (list): list of variables\n",
    "        target (string): name of the target variable\n",
    "\n",
    "    Returns:\n",
    "        list: best model list that contains selected variables, best error rate, and best model\n",
    "    \"\"\"\n",
    "    remaining_vars = vars[:]\n",
    "    current_vars = ['constant']\n",
    "    ols = LinearRegression()\n",
    "    X = data[current_vars]\n",
    "\n",
    "    cv = KFold(5, shuffle=True, random_state=23)  \n",
    "\n",
    "    benchmark_error = np.mean(-1*cross_val_score(ols, X, data[target], cv=cv,\n",
    "                                                 scoring='neg_mean_squared_error'))\n",
    "    print(' Initial run with only one var (constant term/only bias weight):',\n",
    "          current_vars)\n",
    "    print('      Benchmark error:', benchmark_error)\n",
    "    print('')\n",
    "\n",
    "    # Keep adding the best variables (until no improvement can be made)\n",
    "    for iter in range(len(remaining_vars)):\n",
    "        print('\\033[1m'+ 'Iteration:', iter, '\\033[0m')\n",
    "        error_list = []\n",
    "        # Iterate over all the variables left to add\n",
    "        for var in remaining_vars:\n",
    "            # Modify X according to current iteration\n",
    "            X = data[current_vars + [var]]\n",
    "            # Perform 5-fold CV to get errors\n",
    "            error = np.mean(-1*cross_val_score(ols, X, data[target], cv = cv,\n",
    "                                               scoring = 'neg_mean_squared_error'))\n",
    "            error_list.append(error)\n",
    "            print(' Running model with:', current_vars + [var])\n",
    "            print('      Error:', error)\n",
    "\n",
    "        # Chose the smallest error\n",
    "        min_error = min(error_list)\n",
    "        chosen_col_index = error_list.index(min_error)\n",
    "\n",
    "        # If our current smallest error is smaller than our previous error, than we add a variable\n",
    "        if min_error < benchmark_error:\n",
    "            print('          *** Variable selected:', remaining_vars[chosen_col_index])\n",
    "            print('          *** Min error selected:', min_error)\n",
    "            print('          *** Chose the variable that generated the min error + was lower than previous error')\n",
    "            print('')\n",
    "            # Add the variable that produced the smallest error to current_vars\n",
    "            current_vars.append(remaining_vars[chosen_col_index])\n",
    "            # Delete chosen variable from remaining_vars\n",
    "            del remaining_vars[chosen_col_index] \n",
    "            # Update benchmark_error\n",
    "            benchmark_error = min_error\n",
    "\n",
    "        # Otherwise, we stop our model\n",
    "        else:\n",
    "            print('          \\033[4m*** No variable was selected', '\\033[0m')\n",
    "            print('          *** Previous error rate (',\n",
    "                  benchmark_error,') is lower than smallest error rate of this iteration (',\n",
    "                  min_error ,')')\n",
    "            print('          *** Break')\n",
    "            break\n",
    "\n",
    "    print('')\n",
    "    print('Best FSS-5FCV Model Predictors:', current_vars)\n",
    "    print('Error rate:', benchmark_error)\n",
    "    return [current_vars, benchmark_error]\n",
    "\n",
    "fss_5fcv = forward_stepwise_selection_5fcv(boston, vars_left_add, 'CRIM')\n",
    "fss_5fcv_best_model_vars, fss_5fcv_best_model = fss_5fcv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward stepwise with AIC (BSS-AIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best BSS-AIC Model Predictors:: ['constant', 'ZN', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "Best BSS-AIC Model AIC: 3332.3963017429214\n",
      "Best BSS-AIC Model Summary:\n",
      "                             OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   CRIM   R-squared:                       0.445\n",
      "Model:                            OLS   Adj. R-squared:                  0.436\n",
      "Method:                 Least Squares   F-statistic:                     49.78\n",
      "Date:                Tue, 13 Feb 2024   Prob (F-statistic):           7.23e-59\n",
      "Time:                        15:07:31   Log-Likelihood:                -1657.2\n",
      "No. Observations:                 506   AIC:                             3332.\n",
      "Df Residuals:                     497   BIC:                             3370.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "constant      19.7596      6.114      3.232      0.001       7.748      31.771\n",
      "ZN             0.0432      0.018      2.392      0.017       0.008       0.079\n",
      "NOX          -12.9796      4.782     -2.714      0.007     -22.375      -3.585\n",
      "DIS           -0.9301      0.263     -3.535      0.000      -1.447      -0.413\n",
      "RAD            0.5334      0.050     10.678      0.000       0.435       0.632\n",
      "PTRATIO       -0.3164      0.184     -1.721      0.086      -0.677       0.045\n",
      "B             -0.0072      0.004     -1.980      0.048      -0.014   -5.72e-05\n",
      "LSTAT          0.1075      0.070      1.546      0.123      -0.029       0.244\n",
      "MDEV          -0.1765      0.054     -3.254      0.001      -0.283      -0.070\n",
      "==============================================================================\n",
      "Omnibus:                      665.488   Durbin-Watson:                   1.503\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            84815.611\n",
      "Skew:                           6.594   Prob(JB):                         0.00\n",
      "Kurtosis:                      65.040   Cond. No.                     9.34e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.34e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# BSS-AIC\n",
    "\n",
    "# Variables to remove iteratively\n",
    "vars_left_to_drop = ['ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',\n",
    "                    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
    "\n",
    "\n",
    "def backward_stepwise_selection_aic(data, predictors, target):\n",
    "    \"\"\"Return the best model with the lowest AIC using backward stepwise selection.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): entire dataframe\n",
    "        vars (list): list of variables\n",
    "        target (string): name of the target variable\n",
    "\n",
    "    Returns:\n",
    "        list: best model list that contains selected variables, best AIC, and best model\n",
    "    \"\"\"\n",
    "    current_vars = ['constant'] + predictors[:]\n",
    "    best_aic = np.inf\n",
    "    aic_list = []\n",
    "    model_list = []\n",
    "\n",
    "    while len(current_vars) > 0:\n",
    "        aic_candidates = []\n",
    "\n",
    "        for var in current_vars:\n",
    "            temp_vars = current_vars[:]\n",
    "            temp_vars.remove(var)\n",
    "            X_train = sm.add_constant(data[temp_vars])\n",
    "            y_train = data[target]\n",
    "            model = sm.OLS(y_train, X_train).fit()\n",
    "            aic_candidates.append((model.aic, var, model))\n",
    "\n",
    "        aic_candidates.sort()\n",
    "        best_new_aic, removed_var, best_model = aic_candidates[0]\n",
    "\n",
    "        if best_new_aic < best_aic:\n",
    "            best_aic = best_new_aic\n",
    "            current_vars.remove(removed_var)\n",
    "            aic_list.append(best_aic)\n",
    "            model_list.append((current_vars[:], best_model))\n",
    "        else:\n",
    "            # If no improvement, stop the process\n",
    "            break\n",
    "\n",
    "    # Return the history of AICs and models\n",
    "    return model_list[-1]\n",
    "\n",
    "\n",
    "bss_aic = backward_stepwise_selection_aic(boston, vars_left_to_drop, 'CRIM')\n",
    "\n",
    "# Displaying the best model\n",
    "bss_aic_best_model_vars, bss_aic_best_model = bss_aic\n",
    "print(\"Best BSS-AIC Model Predictors::\", bss_aic_best_model_vars)\n",
    "print(\"Best BSS-AIC Model AIC:\", bss_aic_best_model.aic)\n",
    "print(\"Best BSS-AIC Model Summary:\\n\", bss_aic_best_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward stepwise with 5FCV (BSS-5FCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initial run with all vars: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Benchmark error: 43.90194865511775\n",
      "\n",
      "\u001b[1mIteration: 0 \u001b[0m\n",
      " Running model with: ['constant', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 44.42441330332993\n",
      " Running model with: ['constant', 'ZN', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 43.92995670932905\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 43.90331400716521\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 44.24334151226175\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 43.836679592660445\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 43.830625021709565\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 44.95327668573687\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 47.62381784795322\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'PTRATIO', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 43.893032378307325\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'B', 'LSTAT', 'MDEV']\n",
      "      Error: 44.07943862397504\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.087063354234374\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'MDEV']\n",
      "      Error: 44.03375761481523\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
      "      Error: 44.81494435735654\n",
      "          *** Will drop: B\n",
      "          *** Min error selected: 42.087063354234374\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 1 \u001b[0m\n",
      " Running model with: ['constant', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.6035195929099\n",
      " Running model with: ['constant', 'ZN', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.114219566024374\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.099911912264226\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.37923247293754\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.05498680955071\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.04671309932375\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 43.14426557890695\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 46.021552978653474\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.08266176198762\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'LSTAT', 'MDEV']\n",
      "      Error: 42.31313524239458\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'MDEV']\n",
      "      Error: 42.29944804285345\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']\n",
      "      Error: 43.0969591126331\n",
      "          *** Will drop: AGE\n",
      "          *** Min error selected: 42.04671309932375\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 2 \u001b[0m\n",
      " Running model with: ['constant', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.5597401992811\n",
      " Running model with: ['constant', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.07085098575144\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.05231650923414\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.36894370140707\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.01828777442638\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 43.184665661137274\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 45.99900979348841\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.04083444040231\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'LSTAT', 'MDEV']\n",
      "      Error: 42.278585972820245\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'MDEV']\n",
      "      Error: 42.27124926468469\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']\n",
      "      Error: 43.039119057010986\n",
      "          *** Will drop: RM\n",
      "          *** Min error selected: 42.01828777442638\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 3 \u001b[0m\n",
      " Running model with: ['constant', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.56472957636126\n",
      " Running model with: ['constant', 'ZN', 'CHAS', 'NOX', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.04998951207562\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'NOX', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.038941482738885\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.30418208362175\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 43.18596838187152\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'TAX', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 46.07805383972235\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.01604149220564\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'TAX', 'LSTAT', 'MDEV']\n",
      "      Error: 42.2586914647689\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'MDEV']\n",
      "      Error: 42.18152266431218\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']\n",
      "      Error: 42.99347774658539\n",
      "          *** Will drop: TAX\n",
      "          *** Min error selected: 42.01604149220564\n",
      "          *** Chose the variable that generated the min error + was lower than previous error\n",
      "\n",
      "\u001b[1mIteration: 4 \u001b[0m\n",
      " Running model with: ['constant', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.54667675820191\n",
      " Running model with: ['constant', 'ZN', 'CHAS', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.10463200627655\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.027180979975796\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 42.3183229268414\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 43.205497503408836\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "      Error: 52.973611519100395\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'LSTAT', 'MDEV']\n",
      "      Error: 42.25839598822081\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'MDEV']\n",
      "      Error: 42.20536174875614\n",
      " Running model with: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'LSTAT']\n",
      "      Error: 42.97168873438268\n",
      "          \u001b[4m*** No variable was selected \u001b[0m\n",
      "          *** Previous error rate ( 42.01604149220564 ) is lower than smallest error rate of this iteration ( 42.027180979975796 )\n",
      "          *** Break\n",
      "\n",
      "Best BSS-5FCV Model Predictors: ['constant', 'ZN', 'INDUS', 'CHAS', 'NOX', 'DIS', 'RAD', 'PTRATIO', 'LSTAT', 'MDEV']\n",
      "Error rate: 42.01604149220564\n"
     ]
    }
   ],
   "source": [
    "# BSS-5FCV\n",
    "\n",
    "def backward_stepwise_selection_5fcv(data, vars, target):\n",
    "    \"\"\"Return the best model with the lowest 5fcv using backward stepwise selection.\n",
    "\n",
    "    Args:\n",
    "        data (dataframe): entire dataframe\n",
    "        vars (list): list of variables\n",
    "        target (string): name of the target variable\n",
    "\n",
    "    Returns:\n",
    "        list: best model list that contains selected variables, best error rate, and best model\n",
    "    \"\"\"\n",
    "    remaining_vars = vars[:]\n",
    "    current_vars = ['constant'] + remaining_vars\n",
    "    ols = LinearRegression()\n",
    "    X = data[current_vars]\n",
    "\n",
    "    cv = KFold(5, shuffle=True, random_state=23)  \n",
    "\n",
    "    benchmark_error = np.mean(-1*cross_val_score(ols, X, data[target], cv=cv,\n",
    "                                                scoring = 'neg_mean_squared_error'))\n",
    "    print(' Initial run with all vars:', current_vars)\n",
    "    print('      Benchmark error:', benchmark_error)\n",
    "    print('')\n",
    "\n",
    "    # Keep removing the worst variables (until no improvement can be made)\n",
    "    for iter in range(len(remaining_vars)):\n",
    "        print('\\033[1m'+ 'Iteration:', iter, '\\033[0m')\n",
    "        error_list = []\n",
    "        # Iterate over all the variables left to remove\n",
    "        for var in remaining_vars:\n",
    "            # Modify X according to current iteration\n",
    "            vars_to_be_used = ['constant'] + [i for i in remaining_vars if i != var]\n",
    "            X = data[['constant'] + [i for i in remaining_vars if i != var]]\n",
    "            # Perform 5-fold CV to get errors\n",
    "            error = np.mean(-1*cross_val_score(ols, X, data[target], cv = cv,\n",
    "                                               scoring = 'neg_mean_squared_error'))\n",
    "            error_list.append(error)\n",
    "            print(' Running model with:', vars_to_be_used)\n",
    "            print('      Error:', error)\n",
    "\n",
    "        # Chose the largest error\n",
    "        min_error = min(error_list)\n",
    "        chosen_col_index = error_list.index(min_error)\n",
    "\n",
    "        # If our current smallest error is smaller than our previous error, then we drop the variable associated with it\n",
    "        if min_error < benchmark_error:\n",
    "            print('          *** Will drop:', remaining_vars[chosen_col_index])\n",
    "            print('          *** Min error selected:', min_error)\n",
    "            print('          *** Chose the variable that generated the min error + was lower than previous error')\n",
    "            print('')\n",
    "            # Delete chosen variable from current_vars and remaining_vars\n",
    "            del current_vars[chosen_col_index + 1]\n",
    "            del remaining_vars[chosen_col_index]\n",
    "            # Update benchmark_error\n",
    "            benchmark_error = min_error\n",
    "        \n",
    "        # If not, we keep our model\n",
    "        else:\n",
    "            print('          \\033[4m*** No variable was selected', '\\033[0m')\n",
    "            print('          *** Previous error rate (',\n",
    "                  benchmark_error,') is lower than smallest error rate of this iteration (',\n",
    "                  min_error ,')')\n",
    "            print('          *** Break')\n",
    "            break\n",
    "\n",
    "    print('')\n",
    "    print('Best BSS-5FCV Model Predictors:', current_vars)\n",
    "    print('Error rate:', benchmark_error)\n",
    "    return [current_vars, benchmark_error]\n",
    "\n",
    "\n",
    "bss_5fcv = backward_stepwise_selection_5fcv(boston, vars_left_to_drop, 'CRIM')\n",
    "bss_5fcv_best_model_vars, bss_5fcv_best_model = bss_5fcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> \n",
    "\n",
    "By using Leave One Out Cross-Validation on the four models from the above to assess model performance, the model selected through 5FCV minimization shows a lower MSE (42.439) compared to models using AIC (42.995), indicating that it has achieved a good balance between bias and variance.  This model select the following 9 variables (if we drop constant): ['RAD', 'LSTAT', 'MDEV', 'PTRATIO', 'ZN', 'DIS', 'NOX', 'INDUS', 'CHAS']. It is not surprising that 5FCV gives the best result since it is using cross-validation similar to Leave One Out Cross-Validation approach while Akaike informa- tion criterion (AIC). \n",
    "\n",
    "As we discussed earlier, this outcome suggests a strong consensus between the forward and backward selection approaches under the 5FCV or AIC criterion regarding which variables are most important for predicting the per capita crime rate in the Boston dataset. The consistency in variable selection across both methods reinforces the reliability of these variables in modeling the crime rate, despite the inherent differences in the FSS and BSS methodologies.\n",
    "\n",
    "Using LOOCV, MSEs for each model:\n",
    "\n",
    "- FSS-AIC: 42.99482\n",
    "\n",
    "- FSS-5FCV: 42.43918\n",
    "\n",
    "- BSS-AIC: 42.99482\n",
    "\n",
    "- BSS-5FCV: 42.43918\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iternation Milestones:...\n",
      "0\n",
      "\n",
      "[1.2845590250509065, 1.5209451136380667, 16.813111161336856, 21.72260543630353, 24.747257342005703, 12.609986268201185, 0.30472302763454906, 0.010151612077495816, 10.024647784307707, 0.6295305424967303]\n",
      "\n",
      "MSE using best model from FSS-AIC: 42.99482\n"
     ]
    }
   ],
   "source": [
    "# FSS-AIC model, error rate using LOOCV\n",
    "squared_errors = []\n",
    "\n",
    "# Iterate over the entire dataset one observation at a time\n",
    "print('Iternation Milestones:...')\n",
    "for i in boston.index:\n",
    "    # All except observation i is our training set\n",
    "    train = boston.iloc[boston.index != i,:]\n",
    "    # Only observation i is our test set\n",
    "    test = boston.iloc[boston.index == i,:]\n",
    "\n",
    "    # Fit the model and gather the squared error\n",
    "    ols = LinearRegression().fit(train[fss_aic_best_model_vars], train['CRIM'])\n",
    "    test_predicted = ols.predict(test[fss_aic_best_model_vars])\n",
    "    test_actual = test[['CRIM']]\n",
    "    squared_error = np.power(test_predicted - test_actual, 2)\n",
    "\n",
    "    # Store the squared error\n",
    "    squared_errors.append(squared_error.values[0][0])\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(f'\\n{squared_errors[:10]}\\n')\n",
    "\n",
    "# From the squared errors, get the Mean Squared Error (MSE)\n",
    "print(f'MSE using best model from FSS-AIC: {round(np.mean(squared_errors), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iternation Milestones:...\n",
      "0\n",
      "\n",
      "[0.1999346270056265, 1.310230599235957, 18.515001163002044, 20.306044925952737, 23.18055553315749, 10.90091345587461, 0.6055271890566275, 0.1065283555189365, 13.048082605909899, 0.9504486755920881]\n",
      "\n",
      "MSE using best model from FSS-5FCV: 42.43918\n"
     ]
    }
   ],
   "source": [
    "# FSS-5FCV model, error rate using LOOCV\n",
    "squared_errors = []\n",
    "\n",
    "# Iterate over the entire dataset one observation at a time\n",
    "print('Iternation Milestones:...')\n",
    "for i in boston.index:\n",
    "    # All except observation i is our training set\n",
    "    train = boston.iloc[boston.index != i,:]\n",
    "    # Only observation i is our test set\n",
    "    test = boston.iloc[boston.index == i,:]\n",
    "\n",
    "    # Fit the model and gather the squared error\n",
    "    ols = LinearRegression().fit(train[fss_5fcv_best_model_vars], train['CRIM'])\n",
    "    test_predicted = ols.predict(test[fss_5fcv_best_model_vars])\n",
    "    test_actual = test[['CRIM']]\n",
    "    squared_error = np.power(test_predicted - test_actual, 2)\n",
    "\n",
    "    # Store the squared error\n",
    "    squared_errors.append(squared_error.values[0][0])\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(f'\\n{squared_errors[:10]}\\n')\n",
    "\n",
    "# From the squared errors, get the Mean Squared Error (MSE)\n",
    "print(f'MSE using best model from FSS-5FCV: {round(np.mean(squared_errors), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iternation Milestones:...\n",
      "0\n",
      "\n",
      "[1.2845590250510837, 1.520945113638023, 16.813111161337, 21.72260543630317, 24.747257342005597, 12.609986268201563, 0.30472302763456866, 0.010151612077494385, 10.024647784308, 0.6295305424966458]\n",
      "\n",
      "MSE using best model from BSS-AIC: 42.99482\n"
     ]
    }
   ],
   "source": [
    "# BSS-AIC model, error rate using LOOCV\n",
    "squared_errors = []\n",
    "\n",
    "# Iterate over the entire dataset one observation at a time\n",
    "print('Iternation Milestones:...')\n",
    "for i in boston.index:\n",
    "    # All except observation i is our training set\n",
    "    train = boston.iloc[boston.index != i,:]\n",
    "    # Only observation i is our test set\n",
    "    test = boston.iloc[boston.index == i,:]\n",
    "\n",
    "    # Fit the model and gather the squared error\n",
    "    ols = LinearRegression().fit(train[bss_aic_best_model_vars], train['CRIM'])\n",
    "    test_predicted = ols.predict(test[bss_aic_best_model_vars])\n",
    "    test_actual = test[['CRIM']]\n",
    "    squared_error = np.power(test_predicted - test_actual, 2)\n",
    "\n",
    "    # Store the squared error\n",
    "    squared_errors.append(squared_error.values[0][0])\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(f'\\n{squared_errors[:10]}\\n')\n",
    "\n",
    "# From the squared errors, get the Mean Squared Error (MSE)\n",
    "print(f'MSE using best model from BSS-AIC: {round(np.mean(squared_errors), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iternation Milestones:...\n",
      "0\n",
      "\n",
      "[0.19993462700563921, 1.310230599235884, 18.51500116300201, 20.306044925952673, 23.180555533157115, 10.900913455874587, 0.605527189056644, 0.10652835551894345, 13.048082605910027, 0.9504486755921054]\n",
      "\n",
      "MSE using best model from BSS-5FCV: 42.43918\n"
     ]
    }
   ],
   "source": [
    "# BSS-5FCV model, error rate using LOOCV\n",
    "squared_errors = []\n",
    "\n",
    "# Iterate over the entire dataset one observation at a time\n",
    "print('Iternation Milestones:...')\n",
    "for i in boston.index:\n",
    "    # All except observation i is our training set\n",
    "    train = boston.iloc[boston.index != i,:]\n",
    "    # Only observation i is our test set\n",
    "    test = boston.iloc[boston.index == i,:]\n",
    "\n",
    "    # Fit the model and gather the squared error\n",
    "    ols = LinearRegression().fit(train[bss_5fcv_best_model_vars], train['CRIM'])\n",
    "    test_predicted = ols.predict(test[bss_5fcv_best_model_vars])\n",
    "    test_actual = test[['CRIM']]\n",
    "    squared_error = np.power(test_predicted - test_actual, 2)\n",
    "\n",
    "    # Store the squared error\n",
    "    squared_errors.append(squared_error.values[0][0])\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(f'\\n{squared_errors[:10]}\\n')\n",
    "\n",
    "# From the squared errors, get the Mean Squared Error (MSE)\n",
    "print(f'MSE using best model from BSS-5FCV: {round(np.mean(squared_errors), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Does your chosen model involve all of the features in the dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> No, our best model only used 9 features, because based on error calculation, it provides the best accuracy. This phenomenon, where fewer variables lead to lower MSE, aligns with the principle of parsimony in statistical modeling — the idea that among models with similar predictive capabilities, the simpler model is preferred. By focusing on a subset of significant predictors, the model avoids fitting to the noise in the data, leading to better generalization and lower MSE on new data. This also reflects a result of balancing between bias and variance--while bias decrease as number of predictors increase, variance may increase and incease resulting errors, which means more flexibility and more predictors are not necessarily better for reducing errors. See part (a) for more of my previous discussions on significant variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
